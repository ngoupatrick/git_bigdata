- install vm (22.04.5)
	lsb_release -a
		cat /etc/os-release

- ubuntu cmd:
	apt-cache search <keyword>
		apt-cache show <package-name> -> package details
	apt list --installed
	apt list --upgradable
		snap list

- check sshd:
	sudo systemctl status sshd
		sudo systemctl restart ssh.service
	install: sudo apt install openssh-client openssh-server -y

- keys:
	genarate key on client:
		ssh-keygen -f [path_to_file] -t [ecdsa, rsa, dsa] -b 4096
	copy generated public key to remote user (remote host)
		ssh-copy-id [-i path_pub_key] [user]@[remote_host]
	ssh connexion:
		ssh -i path_private_id_rsa [user]@[remote_host]

- install java:
	11: sudo apt install openjdk-11-jre-headless -y
		sudo apt install openjdk-11-jdk -y
		sudo apt install default-jre -y
	17: sudo apt install openjdk-17-jre-headless -y

	check java:
		java --version
		javac --version
		which sshd|ssh|java
		sudo update-alternatives --config java

- install kafka
	url:
		https://kafka.apache.org/quickstart
		https://hevodata.com/blog/how-to-install-kafka-on-ubuntu/

	wget https://dlcdn.apache.org/kafka/4.0.0/kafka_2.13-4.0.0.tgz
	tar -xzf kafka_2.13-4.0.0.tgz
	mv kafka_2.13-4.0.0 kafka
	cd kafka

	Generate a Cluster UUID
		KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
			KAFKA_CLUSTER_ID=YhrKEFNbTiCNn6cDiyruFw
	Format Log Directories:
		mkdir -p ~/data/files
		create folders ~/data/zookeeper/data,log; ~/data/kafka/data,log
		change log and data path in config/server.properties (dataDir=~/data/zookeeper/data
dataLogDir=~/data/zookeeper/log, log.dirs|log.dir=~/data/kafka/log)
		bin/kafka-storage.sh format --standalone -t $KAFKA_CLUSTER_ID -c config/server.properties
	Start the Kafka Server
		bin/kafka-server-start.sh config/server.properties
		bin/kafka-server-stop.sh config/server.properties
			[nohup ./bin/zookeeper-server-start.sh ./config/zookeeper.properties &]
			nohup ./bin/kafka-server-start.sh ./config/server.properties &

	create topic:
		bin/kafka-topics.sh --create --topic topic-test --bootstrap-server localhost:9092 [ --replication-factor 1 --partitions 1 --config cleanup.policy=delete --config delete.retention.ms=60000]
	describe:
		bin/kafka-topics.sh --describe --topic topic-test --bootstrap-server localhost:9092
	list:
		bin/kafka-topics.sh --list --bootstrap-server localhost:9092
	write:
		bin/kafka-console-producer.sh --topic topic-test --bootstrap-server localhost:9092
	read:
		bin/kafka-console-consumer.sh --topic topic-test --from-beginning --bootstrap-server localhost:9092
	delete a topic:
    	bin/kafka-topics.sh --bootstrap-server localhost:9092 --delete --topic topic-test

	read data from file
		set jar:
			echo "plugin.path=libs/connect-file-4.0.0.jar" >> config/connect-standalone.properties
		start connector:
			bin/connect-standalone.sh config/connect-standalone.properties config/connect-file-source.properties config/connect-file-sink.properties
		start consumer (cmd and file):
			bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic connect-test --from-beginning
			more (tail -f) test.sink.txt
		write in file (look in consumer output):
			echo -e "foo\nbar" > test.txt
			echo "Another line" >> test.txt

	read & write mysql (debezium & jdbc):
		read:
			https://medium.com/@kayshowz069/change-data-capture-with-debezium-and-kafka-620f3ed4107a
			install mysql-server:
				sudo apt-get install mysql-server -y
			connect:
				sudo mysql -u root -p
			create db and table:
				create database test_db;
				use test_db;
				create table test_table(id int auto_increment primary key, val varchar(10));
			Create user ‘debezium’ with password ‘dbz’:
				GRANT ALL PRIVILEGES ON *.* TO user_mysql@’%’ IDENTIFIED BY ‘passer’;
					CREATE USER 'user_mysql'@'localhost' IDENTIFIED BY 'passer';
					CREATE USER 'user_mysql'@'%' IDENTIFIED BY 'passer';
					GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'user_mysql'@'localhost';
					GRANT SELECT, RELOAD, SHOW DATABASES, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'user_mysql'@'%';
				FLUSH PRIVILEGES;
			Check Binary Logging Status with the following commands (ON|OFF):
				SHOW variables LIKE'log_bin';
					SELECT variable_value as "BINARY LOGGING STATUS (log-bin) ::" FROM performance_schema.global_variables WHERE variable_name='log_bin';
			change bin log if OFF:
				option1: SET @@binlog_rows_query_log_events=ON; #If the query result is OFF, then you can enable it.
				option 2:
					edit the MySQL configuration file:
						sudo nano /etc/mysql/mysql.conf.d/mysqld.cnf

[mysqld]

# example 1

log_bin = ON
log-bin = mysql-bin
binlog_row_image = FULL
binlog-format = ROW
server-id = 223344
binlog_rows_query_log_events = ON
expire_logs_days = 90
gtid_mode = ON
enforce_gtid_consistency = ON
performance_schema=ON

# Example 2

server-id         = 223344 # Querying variable is called server_id, e.g. SELECT @@server_id; || SELECT variable_value FROM information_schema.global_variables WHERE variable_name='server_id';
log_bin                     = mysql-bin
binlog_format               = ROW
binlog_row_image            = FULL
binlog_expire_logs_seconds  = 864000
binlog_rows_query_log_events=ON

# Set bind address

bind-address = 0.0.0.0
port = 3306
mysqlx-bind-address = 127.0.0.1

			Validating binlog row value options:
				mysql> show global variables where variable_name = 'binlog_row_value_options';
				this variable must be set to a value other than PARTIAL_JSON.
					set @@global.binlog_row_value_options="" ;

			restart mysql:
				sudo service mysql restart
			Setting Up the Debezium MySQL Connector:
				mkdir -p kafka/plugins
				cd kafka/plugins
				wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/1.8.0.Final/debezium-connector-mysql-1.8.0.Final-plugin.tar.gz
					wget https://repo1.maven.org/maven2/io/debezium/debezium-connector-mysql/3.1.1.Final/debezium-connector-mysql-3.1.1.Final-plugin.tar.gz
				tar -xvzf debezium-connector-mysql-1.8.0.Final-plugin.tar.gz
					tar -xvzf debezium-connector-mysql-3.1.1.Final-plugin.tar.gz
				
				#Debezium Connector acts as a Kafka Connector and Kafka stores its connector jars in a specific directory — /				kafka/plugins. 
				mv debezium-connector-mysql-1.8.0.Final-plugin/* plugins
					mv debezium-connector-mysql-3.1.1.Final-plugin/* .					
			add path to properties files:
				nano kafka/config/connect-standalone.properties
					plugin.path=<path_to_plugins>,plugins/
			Copy all jars files in /kafka/plugins to /kafka/libs
				cp -p /plugins/*.jar ./libs/
			Edit mysql connector:
				nano kafka/config/connect-debezium-mysql.properties

name=mysql-connector-02
connector.class=io.debezium.connector.mysql.MySqlConnector
tasks.max=1
database.hostname=localhost
database.port=3306
database.user=user_mysql
database.password=passer
database.server.id=1
database.history.kafka.topic=mysql.history
database.server.name=mysql-connector-02
database.dbname=test_db
table.include.list=test_table
database.history.kafka.bootstrap.servers=localhost:9092
database.allowPublicKeyRetrieval=true

#comma-separated
table.exclude.list
table.include.list
column.exclude.list
column.include.list

# json
curl -i -X PUT -H "Accept:application/json" \
    -H  "Content-Type:application/json" http://localhost:8083/connectors/source-debezium-mysql-00/config \
    -d '
{
    "name": "inventory-connector", 
    "config": {
        "connector.class": "io.debezium.connector.{context}.{connector-name}Connector", 
        "database.hostname": "192.168.99.100", 
        "database.port": "3306", 
        "database.user": "debezium-user", 
        "database.password": "debezium-user-pw", 
        "database.server.id": "184054", 
        "topic.prefix": "fullfillment", 
        "database.include.list": "test_db", 
        "schema.history.internal.kafka.bootstrap.servers": "localhost:9092", 
        "schema.history.internal.kafka.topic": "schemahistory.fullfillment", 
        "include.schema.changes": "true",
        "database.allowPublicKeyRetrieval":"true"
    }
}'

		start services:
			start kafka
			create topic:
				bin/kafka-topics.sh --create --topic msql.history --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --config cleanup.policy=delete --config delete.retention.ms=60000
			start consumer
			Start Kafka Connect Service:
				kafka/bin/connect-standalone.sh kafka/config/connect-standalone.properties kafka/config/connect-debezium-mysql.properties
			cheat sheet:
				url: https://cheatsheetfactory.geekyhacker.com/kafka/kafka-connect-rest
			#You might check whether or not the connection is runing
				curl -s localhost:8083/connectors/mysql-connector/status | jq
		test:
			Login to MySQL To Make Database Changes
		troubleshoot:
			https://rmoff.net/2019/10/23/debezium-mysql-v8-public-key-retrieval-is-not-allowed/
		just capture payload:
			edit connect-debezium-mysql.properties and connect-standalone.properties file

key.converter=org.apache.kafka.connect.json.JsonConverter
key.converter.schemas.enable=false
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
schemas.enable=false


####### Trash #####



		https://github.com/wushujames/kafka-mysql-connector/tree/master
			https://github.com/wushujames/kafka-connect-jdbc/tree/master
			https://medium.com/@alexander.murylev/kafka-connect-debezium-mysql-source-sink-replication-pipeline-fb4d7e9df790
		https://debezium.io/documentation/reference/stable/connectors/mysql.html#setting-up-mysql
			https://debezium.io/documentation/reference/stable/connectors/jdbc.html#jdbc-deployment
		troubleshoot:
			https://groups.google.com/g/debezium/c/rLhOD6dZkhE
			https://karla.tistory.com/11

		https://hevodata.com/learn/mysql-kafka-connector/
		https://debezium.io/documentation/reference/stable/connectors/mysql.html
		https://github.com/confluentinc/demo-scene/tree/master/livestreams/july-15
		https://github.com/confluentinc/demo-scene/tree/master/connect-jdbc

- install hadoop
	url:
		https://shape.host/resources/guide-dinstallation-dapache-hadoop-pour-les-utilisateurs-dubuntu-22-04

- minio:
	url:
		https://www.atlantic.net/dedicated-server-hosting/how-to-deploy-minio-on-ubuntu-22-04-an-open-source-object-storage-application/


